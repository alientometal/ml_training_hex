{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ffb2985-c1e9-4150-8f30-7f3f2290f145",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "0ffb2985-c1e9-4150-8f30-7f3f2290f145"
      },
      "source": [
        "# K-Nearest Neighbors (KNN)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a type of instance-based learning algorithm that can be used for both classification and regression tasks. The principle behind KNN is quite simple: it assumes that similar things exist in close proximity to each other. In other words, similar data points are near each other in the feature space.\n",
        "\n",
        "## How KNN Works?\n",
        "\n",
        "1. **Choose the number of `k` neighbors**: The number `k` in KNN represents the number of nearest neighbors we wish to take a vote from when predicting the label of an unseen data point.\n",
        "\n",
        "2. **Calculate the Distance**: For each data point in the dataset, calculate the distance between the data point and the input. The distance can be Euclidean, Manhattan, Minkowski, etc.\n",
        "\n",
        "3. **Find the Nearest Neighbors**: After calculating the distance, sort them in ascending order and choose the top `k` data points.\n",
        "\n",
        "4. **Make a Decision**:\n",
        "   - **For Classification**: Take a majority vote from the `k` neighbors. The class that has the highest number of votes will be the predicted class for the input data point.\n",
        "   - **For Regression**: Take the average of the `k` neighbors' output values to get the predicted output for the input data point.\n",
        "\n",
        "In the next sections, we'll dive deeper into the practical implementation of KNN using Python and explore its strengths and weaknesses."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42dc030c-6c9a-4056-beb7-d8601f024ce0",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "42dc030c-6c9a-4056-beb7-d8601f024ce0"
      },
      "source": [
        "# Practical Example with Python\n",
        "\n",
        "In this section, we'll work with a sample dataset to demonstrate the application of the KNN algorithm for classification. We'll use the famous Iris dataset, which contains measurements for 150 iris flowers from three different species.\n",
        "\n",
        "The steps we'll follow are:\n",
        "\n",
        "1. **Data Loading and Visualization**\n",
        "2. **Data Preprocessing**\n",
        "3. **Model Training using KNN**\n",
        "4. **Model Evaluation and Hyperparameter Tuning**\n",
        "\n",
        "Let's start by loading and visualizing the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ecb38b2-97f4-4cb9-8a69-a17a7273ee9f",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "4ecb38b2-97f4-4cb9-8a69-a17a7273ee9f"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "# Visualize the first few rows of the dataset\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11acdf55-7426-409a-ba90-51dd44c7b1f5",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "11acdf55-7426-409a-ba90-51dd44c7b1f5"
      },
      "outputs": [],
      "source": [
        "# Visualize the distribution of the species based on sepal measurements\n",
        "sns.pairplot(iris_df, hue='species', height=2.5)\n",
        "plt.suptitle('Pairplot of Iris Dataset', y=1.02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d336ac5b-d706-4c21-a14b-c44c4568b47e",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "d336ac5b-d706-4c21-a14b-c44c4568b47e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Splitting the dataset into training and testing sets\n",
        "X = iris_df.drop('species', axis=1)\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_scaled[:5], X_test_scaled[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49904fec-695a-4d94-b6e3-f3420e688c2a",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "49904fec-695a-4d94-b6e3-f3420e688c2a"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Training the KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predicting on the test set\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "\n",
        "# Evaluating the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
        "\n",
        "accuracy, classification_rep"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8879a3b9-1588-4045-b44e-dd91b5ba8a09",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "8879a3b9-1588-4045-b44e-dd91b5ba8a09"
      },
      "source": [
        "## KNN for Classification\n",
        "\n",
        "For this demonstration, we'll use the famous Iris dataset, which is commonly used in pattern recognition literature. The dataset contains three classes of 50 instances each, where each class refers to a type of iris plant. Each instance has four attributes:\n",
        "\n",
        "1. Sepal length (cm)\n",
        "2. Sepal width (cm)\n",
        "3. Petal length (cm)\n",
        "4. Petal width (cm)\n",
        "\n",
        "Our goal is to classify the iris plants into one of the three species based on these attributes.\n",
        "\n",
        "Let's start by loading and visualizing the Iris dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5785be9-d1e4-48aa-ae19-1b6e28998a9d",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "c5785be9-d1e4-48aa-ae19-1b6e28998a9d"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "iris_df['species'] = iris.target\n",
        "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
        "\n",
        "# Visualize the dataset\n",
        "sns.pairplot(iris_df, hue='species')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c88bd1-c60a-4c3e-a9b8-6df19a00b2cb",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "08c88bd1-c60a-4c3e-a9b8-6df19a00b2cb"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X = iris_df.drop('species', axis=1)\n",
        "y = iris_df['species']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a KNN classifier\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "confusion = confusion_matrix(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "confusion, report"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84ef314-02a5-447a-b348-c468dd377ae3",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "d84ef314-02a5-447a-b348-c468dd377ae3"
      },
      "source": [
        "## KNN for Regression\n",
        "\n",
        "KNN isn't just limited to classification tasks. It can also be used for regression to predict a continuous value. The principle remains the same, but instead of voting for the most frequent class, the algorithm averages the values of the `k` nearest neighbors to predict a continuous output.\n",
        "\n",
        "For this demonstration, we'll use the \"Hours Studied vs. Exam Score\" dataset that we used earlier for linear regression. We'll train a KNN regressor and compare its performance with the linear regression model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f736c5dc-ba65-4d91-ab4d-24ded8e630c8",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "f736c5dc-ba65-4d91-ab4d-24ded8e630c8"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Train a KNN regressor\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
        "knn_regressor.fit(X_train_regression, y_train_regression)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_regression = knn_regressor.predict(X_test_regression)\n",
        "\n",
        "# Evaluate the regressor\n",
        "mse_knn = mean_squared_error(y_test_regression, y_pred_regression)\n",
        "mse_knn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b259b489-236e-4675-89a4-822c52f0afaa",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "b259b489-236e-4675-89a4-822c52f0afaa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plotting the actual values\n",
        "plt.scatter(X_test_regression, y_test_regression, color='blue', label='Actual Values')\n",
        "\n",
        "# Plotting the linear regression predictions\n",
        "plt.plot(X_test_regression, y_pred_linear, color='red', label='Linear Regression Predictions')\n",
        "\n",
        "# Plotting the KNN regressor predictions\n",
        "plt.scatter(X_test_regression, y_pred_regression, color='green', label='KNN Regressor Predictions')\n",
        "\n",
        "plt.title('Hours Studied vs. Exam Score')\n",
        "plt.xlabel('Hours Studied')\n",
        "plt.ylabel('Exam Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf24c954-88e2-46a8-9b15-868f729ba8cf",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "bf24c954-88e2-46a8-9b15-868f729ba8cf"
      },
      "source": [
        "## Advantages and Disadvantages of KNN\n",
        "\n",
        "Like every algorithm, KNN has its strengths and weaknesses. Let's discuss some of the advantages and disadvantages of the KNN algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cd1b5c7-4825-497b-af0b-ab04573497e3",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "8cd1b5c7-4825-497b-af0b-ab04573497e3"
      },
      "source": [
        "### Advantages of KNN\n",
        "\n",
        "1. **Simplicity**: KNN is straightforward and easy to understand. The algorithm relies on the basic principle that similar data points are close to each other in the feature space.\n",
        "\n",
        "2. **No Training Phase**: KNN is a lazy learner, meaning it doesn't have a training phase. All computations are done during the prediction phase, making the prediction process slower but eliminating the need for training.\n",
        "\n",
        "3. **Adaptability**: KNN makes no assumptions about the underlying data distribution, making it suitable for datasets that don't follow any specific distribution.\n",
        "\n",
        "4. **Multifunctional**: KNN can be used for both classification and regression tasks.\n",
        "\n",
        "5. **Robust to Noisy Data**: With a suitable choice of `k`, KNN can be robust to noisy data since it relies on the majority voting or averaging mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd2b710d-f18b-4b90-a4f2-a09a4a2f5b19",
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "dd2b710d-f18b-4b90-a4f2-a09a4a2f5b19"
      },
      "source": [
        "### Disadvantages of KNN\n",
        "\n",
        "1. **Computationally Intensive**: Since KNN computes distances between data points during the prediction phase, it can be computationally intensive, especially for large datasets.\n",
        "\n",
        "2. **Sensitive to Irrelevant Features**: KNN relies on distances, so it's sensitive to irrelevant or redundant features. Feature scaling and feature selection become crucial when working with KNN.\n",
        "\n",
        "3. **Optimal k Determination**: Choosing the right value of `k` is critical. A small value of `k` can make the model sensitive to noise, while a large value can make it computationally expensive.\n",
        "\n",
        "4. **Storage Requirements**: KNN requires storing the entire dataset, which can be a challenge for large datasets in terms of memory.\n",
        "\n",
        "5. **Categorical Data**: Handling categorical data can be tricky with KNN since it's challenging to define a distance metric for categorical variables."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "noteable": {
      "last_delta_id": "ba11a567-cbc7-4731-97fb-3ca1d0da0c7f"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b445c1de-c6f0-5ee5-81c9-74cd11dbc67f",
        "openai_ephemeral_user_id": "536684a1-e646-5c9b-a034-5af5a91caf6c",
        "openai_subdivision1_iso_code": "MX-MOR"
      }
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "selected_hardware_size": "small",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}