{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29b1c1e2-30a6-4cdd-b30a-b95f52e730aa",
      "metadata": {
        "id": "29b1c1e2-30a6-4cdd-b30a-b95f52e730aa",
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "# Support Vector Machines (SVM)\n",
        "\n",
        "Support Vector Machines (SVM) are a type of supervised machine learning algorithm that can be used for both classification and regression tasks. However, they are more commonly used for classification problems. SVMs are known for their ability to handle high-dimensional data and their versatility in modeling complex, non-linear decision boundaries.\n",
        "\n",
        "In this tutorial, we'll explore the fundamentals of SVM, understand the mathematics behind it, and see it in action with Python."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41008b6-6e84-45fb-ab35-267a89b3ba53",
      "metadata": {
        "id": "e41008b6-6e84-45fb-ab35-267a89b3ba53",
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Linear SVM\n",
        "\n",
        "In the context of SVM, when we talk about a linear SVM, we refer to the scenario where the data is linearly separable. This means that the two classes can be separated by a straight line (in 2D), a plane (in 3D), or a hyperplane (in higher dimensions).\n",
        "\n",
        "The goal of the SVM algorithm is to find the hyperplane that best separates the classes. This is the hyperplane for which the margin, the distance between the hyperplane and the nearest data point from either class, is maximized.\n",
        "\n",
        "Mathematically, if our hyperplane is defined by the equation $w^T x + b = 0$, then the objective of the SVM is to maximize the margin $\\frac{2}{||w||}$ while keeping the constraints that the samples are classified correctly.\n",
        "\n",
        "Let's visualize this with a simple example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83a0687e-04d2-43c5-a42a-610c9778bb79",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "83a0687e-04d2-43c5-a42a-610c9778bb79",
        "noteable": {
          "cell_type": "code"
        },
        "outputId": "f61dc1ec-12b1-4e50-efe6-5671aff0ddf7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=100, centers=2, random_state=6)\n",
        "\n",
        "# Fit the SVM model\n",
        "clf = SVC(kernel='linear', C=1000)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot the decision boundary\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# Create grid to evaluate model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "Z = clf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# Plot decision boundary and margins\n",
        "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "# Plot support vectors\n",
        "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
        "plt.title('Linear SVM with Decision Boundary and Support Vectors')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c7aeeb2-558a-4686-8f4f-7b07277b68aa",
      "metadata": {
        "id": "5c7aeeb2-558a-4686-8f4f-7b07277b68aa",
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Kernel SVM\n",
        "\n",
        "While linear SVM works well for linearly separable data, real-world data is often not linearly separable. This is where the Kernel SVM comes into play.\n",
        "\n",
        "The idea behind Kernel SVM is to transform the input data into a higher-dimensional space where it becomes linearly separable. This transformation is done using a kernel function. Once the data is transformed, we can then use a linear SVM to find the decision boundary.\n",
        "\n",
        "There are several kernel functions used in practice:\n",
        "- **Linear Kernel**: $K(x, x') = x^T x'$\n",
        "- **Polynomial Kernel**: $K(x, x') = (1 + x^T x')^d$\n",
        "- **Radial Basis Function (RBF) or Gaussian Kernel**: $K(x, x') = e^{-\\gamma ||x - x'||^2}$\n",
        "- **Sigmoid Kernel**: $K(x, x') = \\tanh(\\alpha x^T x' + c)$\n",
        "\n",
        "Among these, the RBF kernel is the most commonly used. It can handle complex decision boundaries and works well in many scenarios.\n",
        "\n",
        "Let's visualize how the Kernel SVM works using the RBF kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7041530b-590f-44cf-9779-efc4bf1faa35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "7041530b-590f-44cf-9779-efc4bf1faa35",
        "noteable": {
          "cell_type": "code"
        },
        "outputId": "955e4504-e038-473a-d7a9-9063806dfa60"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_circles\n",
        "\n",
        "# Generate synthetic data with concentric circles\n",
        "X, y = make_circles(n_samples=100, factor=0.3, noise=0.05, random_state=42)\n",
        "\n",
        "# Visualize the data\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "plt.title('Synthetic Data with Concentric Circles')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c93f513-613f-4744-ab7f-4f11a3cc2624",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "1c93f513-613f-4744-ab7f-4f11a3cc2624",
        "noteable": {
          "cell_type": "code"
        },
        "outputId": "b7160595-319a-4c9a-c69c-fa842ceb63c0"
      },
      "outputs": [],
      "source": [
        "# Fit the SVM model with RBF kernel\n",
        "clf_rbf = SVC(kernel='rbf', C=1000)\n",
        "clf_rbf.fit(X, y)\n",
        "\n",
        "# Plot the data points\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "\n",
        "# Plot the decision boundary\n",
        "ax = plt.gca()\n",
        "xlim = ax.get_xlim()\n",
        "ylim = ax.get_ylim()\n",
        "\n",
        "# Create grid to evaluate model\n",
        "xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "YY, XX = np.meshgrid(yy, xx)\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "Z = clf_rbf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "# Plot decision boundary and margins\n",
        "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "# Plot support vectors\n",
        "ax.scatter(clf_rbf.support_vectors_[:, 0], clf_rbf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
        "plt.title('Kernel SVM with RBF Kernel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40033224-339a-4a07-b673-b9e9a5fd7610",
      "metadata": {
        "id": "40033224-339a-4a07-b673-b9e9a5fd7610",
        "noteable": {
          "cell_type": "markdown"
        }
      },
      "source": [
        "## Soft Margin and Hyperparameter Tuning\n",
        "\n",
        "In real-world scenarios, data is often noisy and may contain outliers. In such cases, strictly maximizing the margin (as in a hard margin SVM) might not be ideal, as it can lead to a model that overfits to the training data. This is where the concept of a soft margin comes into play.\n",
        "\n",
        "A soft margin SVM allows some misclassifications in order to achieve a better generalization to unseen data. The degree to which misclassifications are allowed is controlled by a hyperparameter, often denoted as `C`.\n",
        "\n",
        "- **High C value**: Implies a smaller margin, which might result in a lower training error but a higher test error (potential overfitting).\n",
        "- **Low C value**: Implies a larger margin, allowing some misclassifications in the training data for better generalization.\n",
        "\n",
        "Tuning the `C` parameter is crucial for achieving the best performance with SVM. Similarly, when using Kernel SVM, the choice of kernel and its parameters (e.g., `gamma` for the RBF kernel) also need to be tuned.\n",
        "\n",
        "Hyperparameter tuning can be done using techniques like grid search or random search combined with cross-validation.\n",
        "\n",
        "Let's see the effect of the `C` parameter on the decision boundary using our synthetic dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee0ad707-1ea7-479c-855f-e9e5e12a26f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "ee0ad707-1ea7-479c-855f-e9e5e12a26f8",
        "noteable": {
          "cell_type": "code"
        },
        "outputId": "e4bbe227-f605-40ee-c507-1ebb9a9cca7c"
      },
      "outputs": [],
      "source": [
        "C_values = [0.1, 1, 10, 100]\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "for i, C in enumerate(C_values, 1):\n",
        "    # Fit the SVM model with RBF kernel and different C values\n",
        "    clf_rbf = SVC(kernel='rbf', C=C)\n",
        "    clf_rbf.fit(X, y)\n",
        "\n",
        "    plt.subplot(2, 2, i)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    # Create grid to evaluate model\n",
        "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "    YY, XX = np.meshgrid(yy, xx)\n",
        "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "    Z = clf_rbf.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "    # Plot decision boundary and margins\n",
        "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "    # Plot support vectors\n",
        "    ax.scatter(clf_rbf.support_vectors_[:, 0], clf_rbf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
        "    plt.title(f'Kernel SVM with RBF Kernel (C={C})')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b445c1de-c6f0-5ee5-81c9-74cd11dbc67f",
        "openai_ephemeral_user_id": "10ef5ec9-cae8-51f6-9563-95eee35667ea",
        "openai_subdivision1_iso_code": "MX-MOR"
      }
    },
    "selected_hardware_size": "small"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
