{"cells":[{"cell_type":"markdown","source":["# Linear Regression\n","\n","In this tutorial, we will cover the foundational concepts of machine learning and then delve into a practical example using simple linear regression. The aim is to provide a clear understanding of the machine learning process, from data collection to model evaluation.\n","\n","## Table of Contents\n","1. Introduction to Machine Learning\n","2. Simple Linear Regression\n","   - Theory and Mathematical Formulation\n","   - Practical Example with Python\n","3. Summary and Key Takeaways"],"metadata":{"noteable":{"cell_type":"markdown"},"id":"26d9386d-6948-4040-98fa-a1235e90f9dc"},"id":"26d9386d-6948-4040-98fa-a1235e90f9dc"},{"cell_type":"markdown","source":["## 1. Introduction to Machine Learning\n","\n","Machine learning is a subset of artificial intelligence (AI) that focuses on building systems that can learn from data. Instead of being explicitly programmed to perform a task, a machine learning model uses patterns in data to make decisions and predictions.\n","\n","### Types of Machine Learning:\n","- **Supervised Learning**: The model is trained on a labeled dataset, meaning the training data includes the desired solution, known as a label. The goal is to learn a mapping from inputs to outputs. Examples include regression and classification problems.\n","- **Unsupervised Learning**: The model is trained on an unlabeled dataset, meaning the training data does not include labels. The goal is to find patterns or relationships in the data. Examples include clustering and association.\n","- **Reinforcement Learning**: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties. The goal is to learn a strategy to maximize the cumulative reward over time.\n","\n","### Key Concepts:\n","- **Training Data**: The data on which the model is trained. It includes both the input data and the corresponding desired output.\n","- **Testing Data**: Data that is separate from the training data and is used to evaluate the performance of the model.\n","- **Features**: Input variables that the model uses to make predictions.\n","- **Target/Label**: The output variable that the model aims to predict (in supervised learning).\n","- **Model**: A mathematical representation of a real-world process. In machine learning, a model is the output of a training algorithm, which is used to make predictions.\n","- **Training**: The process of adjusting a model's parameters to fit the training data.\n","- **Prediction**: Using the trained model to make predictions on new, unseen data.\n","- **Evaluation**: Assessing the performance of a trained model using certain metrics (e.g., accuracy, mean squared error).\n","\n","With this foundational knowledge, we can now delve into a practical example using simple linear regression."],"metadata":{"noteable":{"cell_type":"markdown"},"id":"54c9a23d-3684-452a-9185-0172b0ceba78"},"id":"54c9a23d-3684-452a-9185-0172b0ceba78"},{"cell_type":"markdown","source":["## 2. Simple Linear Regression\n","\n","Simple linear regression is a linear approach to modeling the relationship between a dependent variable and one independent variable. The goal is to find the best line (in terms of least squares error) that fits the data.\n","\n","### Theory and Mathematical Formulation:\n","\n","The relationship between the dependent variable y and the independent variable x is represented as:\n","\n","y = β0 + β1 x + ε\n","\n","Where:\n","- y is the dependent variable (what we're trying to predict).\n","- x is the independent variable (the input).\n","- β0 is the y-intercept.\n","- β1 is the slope of the line.\n","- ε is the error term (difference between observed and predicted values).\n","\n","The goal of simple linear regression is to find the values of β0 and β1 that minimize the sum of the squared differences between the observed values (actual values) and the values predicted by the model.\n","\n","In the next section, we'll implement a practical example of simple linear regression using Python."],"metadata":{"noteable":{"cell_type":"markdown"},"id":"661e9bc2-1569-4fa4-aab9-1b29b5e1755c"},"id":"661e9bc2-1569-4fa4-aab9-1b29b5e1755c"},{"cell_type":"markdown","source":["### Practical Example with Python\n","\n","For this example, we'll use a simple dataset that contains information about the relationship between the number of hours studied and the scores obtained in an exam. Our goal is to predict the exam score based on the number of hours studied.\n","\n","#### Data Loading and Visualization\n","\n","Let's start by importing the necessary libraries and loading the data."],"metadata":{"noteable":{"cell_type":"markdown"},"id":"cd066202-7c6a-460e-bff2-a9ba10b0a507"},"id":"cd066202-7c6a-460e-bff2-a9ba10b0a507"},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","# Generate a synthetic dataset\n","np.random.seed(42)\n","hours_studied = np.random.rand(100) * 10\n","exam_scores = 5 + 2.5 * hours_studied + np.random.randn(100) * 2\n","\n","# Convert to DataFrame for easier handling\n","data = pd.DataFrame({'Hours_Studied': hours_studied, 'Exam_Score': exam_scores})\n","\n","# Visualize the data\n","plt.figure(figsize=(10, 6))\n","plt.scatter(data['Hours_Studied'], data['Exam_Score'], color='blue', label='Data points')\n","plt.title('Relationship between Hours Studied and Exam Score')\n","plt.xlabel('Hours Studied')\n","plt.ylabel('Exam Score')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"f36a001a-dead-4c4c-b80a-d62b372ca634"},"id":"f36a001a-dead-4c4c-b80a-d62b372ca634"},{"cell_type":"code","source":["# Splitting the data into training and testing sets\n","X = data[['Hours_Studied']]\n","y = data['Exam_Score']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Training the linear regression model\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","\n","# Making predictions\n","y_pred = regressor.predict(X_test)\n","\n","# Visualizing the training set results\n","plt.figure(figsize=(10, 6))\n","plt.scatter(X_train, y_train, color='blue', label='Training data')\n","plt.scatter(X_test, y_test, color='red', label='Testing data')\n","plt.plot(X_train, regressor.predict(X_train), color='green', label='Regression line')\n","plt.title('Relationship between Hours Studied and Exam Score (Training set)')\n","plt.xlabel('Hours Studied')\n","plt.ylabel('Exam Score')\n","plt.legend()\n","plt.grid(True)\n","plt.show()"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"b11ab2a0-30dc-471b-a0ef-7999951b053f"},"id":"b11ab2a0-30dc-471b-a0ef-7999951b053f"},{"cell_type":"code","source":["# Evaluating the model\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","evaluation_metrics = pd.DataFrame({'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'Root Mean Squared Error', 'R-squared'],\n","                                  'Value': [mae, mse, rmse, r2]})\n","evaluation_metrics"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"9095402d-f9a5-41e6-9046-a4b5d0470a94"},"id":"9095402d-f9a5-41e6-9046-a4b5d0470a94"},{"cell_type":"markdown","source":["## Gradient Descent for Linear Regression\n","\n","Gradient Descent is an optimization algorithm used to minimize the loss function by iteratively adjusting the model's parameters. The idea is to take repeated steps in the opposite direction of the gradient (or slope) of the function at the current point, because this is the direction of steepest descent.\n","\n","For linear regression, our goal is to minimize the Mean Squared Error (MSE) loss function. The gradient descent algorithm updates the parameters (weights and bias) using the following formula:\n","\n","$$\\theta_{new} = \\theta_{old} - \\alpha \\times \\nabla_{\\theta} J(\\theta)$$\n","\n","Where:\n","- $$\\theta $$ are the parameters (weights and bias).\n","- $$ \\alpha $$ is the learning rate, a hyperparameter that determines the step size at each iteration while moving towards a minimum of the loss function.\n","- $$ \\nabla_{\\theta} J(\\theta) $$ is the gradient of the loss function with respect to the parameters.\n","\n","Next, we'll implement gradient descent from scratch and train our linear regression model using this algorithm."],"metadata":{"noteable":{"cell_type":"markdown"},"id":"21fa9b51-0182-4e08-a7f6-69c7e5322217"},"id":"21fa9b51-0182-4e08-a7f6-69c7e5322217"},{"cell_type":"code","source":["def compute_gradient(X, y, theta0, theta1):\n","    m = len(y)\n","    theta0_gradient = -(2/m) * sum(y - (theta0 + theta1*X))\n","    theta1_gradient = -(2/m) * sum(X * (y - (theta0 + theta1*X)))\n","    return theta0_gradient, theta1_gradient\n","\n","def gradient_descent(X, y, learning_rate=0.01, iterations=1000):\n","    theta0 = 0\n","    theta1 = 0\n","    for _ in range(iterations):\n","        theta0_gradient, theta1_gradient = compute_gradient(X, y, theta0, theta1)\n","        theta0 = theta0 - learning_rate * theta0_gradient\n","        theta1 = theta1 - learning_rate * theta1_gradient\n","    return theta0, theta1\n","\n","# Train the model using gradient descent\n","theta0_gd, theta1_gd = gradient_descent(X_train['Hours_Studied'], y_train)\n","theta0_gd, theta1_gd"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"13f9afc0-606e-4702-a7be-3d599a2ff765"},"id":"13f9afc0-606e-4702-a7be-3d599a2ff765"},{"cell_type":"code","source":["# Making predictions using the model trained with gradient descent\n","y_pred_gd = theta0_gd + theta1_gd * X_test['Hours_Studied']\n","\n","# Evaluating the model trained with gradient descent\n","mae_gd = mean_absolute_error(y_test, y_pred_gd)\n","mse_gd = mean_squared_error(y_test, y_pred_gd)\n","rmse_gd = np.sqrt(mse_gd)\n","r2_gd = r2_score(y_test, y_pred_gd)\n","\n","evaluation_metrics_gd = pd.DataFrame({'Metric': ['Mean Absolute Error', 'Mean Squared Error', 'Root Mean Squared Error', 'R-squared'],\n","                                     'Value (Gradient Descent)': [mae_gd, mse_gd, rmse_gd, r2_gd],\n","                                     'Value (Analytical Solution)': [mae, mse, rmse, r2]})\n","evaluation_metrics_gd"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"53933db1-1cd6-4c44-8b2d-910b05d9b2d2"},"id":"53933db1-1cd6-4c44-8b2d-910b05d9b2d2"},{"cell_type":"markdown","source":["# Checkpointing Strategy"],"metadata":{"id":"dZuP-gUpLTkV"},"id":"dZuP-gUpLTkV"},{"cell_type":"code","source":["import pickle\n","import os\n","\n","def save_checkpoint(iteration, theta0, theta1, path='checkpoints'):\n","    \"\"\"Save model parameters as a checkpoint.\"\"\"\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","    checkpoint = {\n","        'iteration': iteration,\n","        'theta0': theta0,\n","        'theta1': theta1\n","    }\n","    checkpoint_path = os.path.join(path, f'checkpoint_{iteration}.pkl')\n","    with open(checkpoint_path, 'wb') as f:\n","        pickle.dump(checkpoint, f)\n","\n","def load_checkpoint(path):\n","    \"\"\"Load model parameters from a checkpoint.\"\"\"\n","    with open(path, 'rb') as f:\n","        checkpoint = pickle.load(f)\n","    return checkpoint['iteration'], checkpoint['theta0'], checkpoint['theta1']\n","\n","def gradient_descent_with_checkpointing(X, y, learning_rate=0.01, iterations=1000, checkpoint_frequency=100):\n","    theta0 = 0\n","    theta1 = 0\n","    for i in range(iterations):\n","        theta0_gradient, theta1_gradient = compute_gradient(X, y, theta0, theta1)\n","        theta0 = theta0 - learning_rate * theta0_gradient\n","        theta1 = theta1 - learning_rate * theta1_gradient\n","        if i % checkpoint_frequency == 0:\n","            save_checkpoint(i, theta0, theta1)\n","    return theta0, theta1\n","\n","# Train the model using gradient descent with checkpointing\n","theta0_gd_checkpointed, theta1_gd_checkpointed = gradient_descent_with_checkpointing(X_train['Hours_Studied'], y_train)\n","theta0_gd_checkpointed, theta1_gd_checkpointed"],"outputs":[],"execution_count":null,"metadata":{"noteable":{"cell_type":"code"},"id":"7fe0e497-407e-4c68-8552-0df19cbc215a"},"id":"7fe0e497-407e-4c68-8552-0df19cbc215a"}],"metadata":{"noteable-chatgpt":{"create_notebook":{"openai_conversation_id":"b445c1de-c6f0-5ee5-81c9-74cd11dbc67f","openai_ephemeral_user_id":"536684a1-e646-5c9b-a034-5af5a91caf6c","openai_subdivision1_iso_code":"MX-MOR"}},"kernel_info":{"name":"python3"},"kernelspec":{"display_name":"Python 3.9","language":"python","name":"python3"},"selected_hardware_size":"small","noteable":{"last_delta_id":"c2e54c56-0fad-40ac-b1ae-2e79052b3563"},"nteract":{"version":"noteable@2.9.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}