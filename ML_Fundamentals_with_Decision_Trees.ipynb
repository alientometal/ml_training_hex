{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        "\n",
        "Decision Trees are a type of supervised learning algorithm that can be used for both classification and regression tasks. They work by recursively splitting the dataset into subsets based on the feature values, resulting in a tree-like model of decisions.\n",
        "\n",
        "In this tutorial, we'll explore the fundamentals of Decision Trees, understand the criteria for splitting the data, and delve into the concepts of overfitting and tree pruning."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "6367bf52-204f-491a-84df-8f3bea43cf47"
      },
      "id": "6367bf52-204f-491a-84df-8f3bea43cf47"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting Criteria\n",
        "\n",
        "When building a decision tree, the algorithm needs to decide which feature to split on at each step in the tree. The quality of a split is determined using certain metrics. Two of the most common metrics used for this purpose are Gini Impurity and Information Gain.\n",
        "\n",
        "### 1. Gini Impurity\n",
        "\n",
        "Gini Impurity measures the disorder of a set of elements. It calculates the amount of probability of a specific attribute that is classified incorrectly when selected randomly. If all the elements are linked with a single class, then we can say it has zero Gini impurity.\n",
        "\n",
        "### 2. Information Gain\n",
        "\n",
        "Information Gain measures the reduction in entropy achieved because of the split. A decision tree algorithm always tries to maximize the Information Gain.\n",
        "\n",
        "Let's delve deeper into these metrics and see them in action."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "8089d119-8f7d-4192-8909-b4ae30c44058"
      },
      "id": "8089d119-8f7d-4192-8909-b4ae30c44058"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def gini_impurity(p):\n",
        "    return 2 * p * (1 - p)\n",
        "\n",
        "def entropy(p):\n",
        "    if p == 0 or p == 1:\n",
        "        return 0\n",
        "    return -p * np.log2(p) - (1 - p) * np.log2(1 - p)\n",
        "\n",
        "# Sample probabilities\n",
        "probabilities = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "gini_values = [gini_impurity(p) for p in probabilities]\n",
        "entropy_values = [entropy(p) for p in probabilities]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(probabilities, gini_values, '-o', label='Gini Impurity')\n",
        "plt.plot(probabilities, entropy_values, '-o', label='Entropy')\n",
        "plt.title('Gini Impurity and Entropy for Different Probabilities')\n",
        "plt.xlabel('Probability of Positive Class')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "80d922c0-e2ad-41dd-8923-ac2e9bff5d77"
      },
      "id": "80d922c0-e2ad-41dd-8923-ac2e9bff5d77"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Decision Tree Classifier\n",
        "\n",
        "To demonstrate the Decision Tree Classifier, we'll use a simple dataset. For this example, we'll use the famous Iris dataset, which contains measurements of 150 iris flowers from three different species: Setosa, Versicolour, and Virginica.\n",
        "\n",
        "We'll build a Decision Tree Classifier to classify the iris flowers based on their measurements."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "fa3c2519-52fc-4b44-aee1-3f7a6b3b6267"
      },
      "id": "fa3c2519-52fc-4b44-aee1-3f7a6b3b6267"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "cd03e744-9aeb-465e-bd82-ba75972e5411"
      },
      "id": "cd03e744-9aeb-465e-bd82-ba75972e5411"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
        "plt.title('Decision Tree Structure')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "cf21ac94-de4f-4f77-bff2-890eeb239864"
      },
      "id": "cf21ac94-de4f-4f77-bff2-890eeb239864"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overfitting and Tree Pruning\n",
        "\n",
        "Decision trees are prone to overfitting, especially when the tree is deep. Overfitting occurs when the model captures noise in the training data and performs poorly on unseen data. One way to combat overfitting in decision trees is through tree pruning.\n",
        "\n",
        "### Tree Pruning\n",
        "\n",
        "Tree pruning involves cutting branches off the decision tree. The idea is to remove the parts of the tree that do not provide power in predicting target values. Pruning can be done in two ways:\n",
        "\n",
        "1. **Pre-pruning**: This involves setting constraints on tree size during its construction. Examples include limiting the maximum depth of the tree or setting a minimum number of samples required to make a split at a node.\n",
        "\n",
        "2. **Post-pruning**: This involves removing branches from a fully grown tree. One approach is to remove branches that give less importance to the overall classifier's performance.\n",
        "\n",
        "Let's see the effect of tree depth on overfitting and how pruning can help."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "11a75563-433e-4e44-93a3-368892919e1a"
      },
      "id": "11a75563-433e-4e44-93a3-368892919e1a"
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "max_depths = list(range(1, 6))\n",
        "\n",
        "# Iterate over different depths to evaluate accuracy\n",
        "for max_depth in max_depths:\n",
        "    # Train a Decision Tree Classifier with varying depth\n",
        "    clf = DecisionTreeClassifier(criterion='gini', max_depth=max_depth, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate on training data\n",
        "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Plotting the accuracies\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depths, train_accuracies, '-o', label='Training Accuracy')\n",
        "plt.plot(max_depths, test_accuracies, '-o', label='Test Accuracy')\n",
        "plt.title('Effect of Tree Depth on Overfitting')\n",
        "plt.xlabel('Max Depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "d6405d56-3bb6-41e3-aa79-496357f95a7a"
      },
      "id": "d6405d56-3bb6-41e3-aa79-496357f95a7a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-pruning\n",
        "\n",
        "Post-pruning, also known as cost complexity pruning, involves pruning branches from a fully grown tree. The idea is to find a subtree that offers the best compromise between fit and complexity. In scikit-learn, this can be achieved using the `ccp_alpha` parameter, which determines the complexity cost.\n",
        "\n",
        "A tree with more leaves will have a greater cost complexity. By increasing the `ccp_alpha` parameter, branches with the highest cost complexity are pruned first, resulting in a smaller tree.\n",
        "\n",
        "Let's visualize a pruned decision tree using a non-zero value for `ccp_alpha`."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "9b9d4229-472c-44f2-8ef3-392c552e5b3a"
      },
      "id": "9b9d4229-472c-44f2-8ef3-392c552e5b3a"
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a pruned Decision Tree Classifier\n",
        "pruned_clf = DecisionTreeClassifier(criterion='gini', ccp_alpha=0.02, random_state=42)\n",
        "pruned_clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the pruned tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(pruned_clf, filled=True, feature_names=iris.feature_names, class_names=iris.target_names, rounded=True)\n",
        "plt.title('Pruned Decision Tree Structure')\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "d2a96c8e-8e63-4156-bc4b-11a5d4745110"
      },
      "id": "d2a96c8e-8e63-4156-bc4b-11a5d4745110"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of Decision Trees\n",
        "\n",
        "Visualizing decision trees is a powerful way to understand how the model makes decisions. It provides a clear, hierarchical structure of decisions, making it one of the most interpretable machine learning models.\n",
        "\n",
        "The visualization we generated above showcases the decisions made at each node, the criteria for those decisions, and the distribution of classes at each node. The leaves of the tree represent the final decisions or predictions made by the model.\n",
        "\n",
        "Here are some key points to note from the visualization:\n",
        "\n",
        "- **Decision Nodes**: These nodes showcase a decision criterion based on a feature value. For example, a node might split the data based on whether the petal width is less than or equal to a certain value.\n",
        "- **Leaves**: These are the terminal nodes that predict the outcome. The color of the node indicates the predicted class, and the shade of the color indicates the confidence of the prediction.\n",
        "- **Gini Impurity**: This value, shown in each node, indicates the impurity of the data at that node. A Gini impurity of 0 means all the data at that node belongs to a single class.\n",
        "\n",
        "Decision tree visualizations can be a valuable tool, especially when explaining the model's decisions to non-technical stakeholders."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "86d1b1d6-721b-4d2e-9ab8-3355f82344c3"
      },
      "id": "86d1b1d6-721b-4d2e-9ab8-3355f82344c3"
    }
  ],
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b445c1de-c6f0-5ee5-81c9-74cd11dbc67f",
        "openai_ephemeral_user_id": "10ef5ec9-cae8-51f6-9563-95eee35667ea",
        "openai_subdivision1_iso_code": "MX-MOR"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "noteable": {
      "last_delta_id": "812d18d0-7ea7-468e-a1f5-718ae381e53f"
    },
    "nteract": {
      "version": "noteable@2.9.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}