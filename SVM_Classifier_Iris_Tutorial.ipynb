{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b445c1de-c6f0-5ee5-81c9-74cd11dbc67f",
        "openai_ephemeral_user_id": "ae887d81-bc3f-5cba-bb4a-0dbd09528325",
        "openai_subdivision1_iso_code": "MX-MOR"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "id": "2e2846f6-841c-4b11-8d15-cc88c6ba26cd",
      "cell_type": "markdown",
      "source": [
        "# SVM Classifier with the Iris Dataset\n",
        "\n",
        "Support Vector Machines (SVM) are powerful supervised machine learning algorithms used for classification and regression tasks. They work by finding a hyperplane that best divides a dataset into classes.\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this tutorial, we will cover:\n",
        "\n",
        "1. Introduction to SVM Classifier\n",
        "2. Understanding the Concept of Hyperplanes and Margins\n",
        "3. SVM with the Iris Dataset\n",
        "4. Evaluating the SVM Classifier\n",
        "5. Advantages and Limitations of SVM\n",
        "\n",
        "Let's start with a brief introduction to SVM Classifier."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "2e2846f6-841c-4b11-8d15-cc88c6ba26cd"
      }
    },
    {
      "id": "189c784d-6d4c-4247-9d1f-de96f9a1fdf8",
      "cell_type": "markdown",
      "source": [
        "## Understanding the Concept of Hyperplanes and Margins\n",
        "\n",
        "A hyperplane is a flat affine subspace of one dimension less than its ambient space. For instance, in a 2-dimensional space, a hyperplane is a line; in a 3-dimensional space, it's a flat plane, and so on.\n",
        "\n",
        "In the context of SVM, a hyperplane is used to separate data points of different classes. The objective is to find the optimal hyperplane that maximizes the margin between two classes.\n",
        "\n",
        "The **margin** is defined as the distance between the nearest data point (of either class) and the hyperplane. The goal of SVM is to maximize this margin. The data points that are closest to the hyperplane and influence its position and orientation are termed as **support vectors**.\n",
        "\n",
        "SVMs can be used in both linear and non-linear classification. For non-linear classification, SVM uses a technique called the kernel trick. The kernel trick involves transforming the input data into a higher-dimensional space where a hyperplane can be used to separate the data.\n",
        "\n",
        "Next, we'll apply the SVM classifier to the Iris dataset and visualize the decision boundaries."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "189c784d-6d4c-4247-9d1f-de96f9a1fdf8"
      }
    },
    {
      "id": "2e61a524-53f9-4f4f-a004-7568848e3543",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "2e61a524-53f9-4f4f-a004-7568848e3543"
      },
      "execution_count": null,
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Extract only the features of interest (sepal length and sepal width) and the relevant classes (setosa and versicolor)\n",
        "X = X[y != 2, :2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Plot the data\n",
        "plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='red', marker='o', label='setosa')\n",
        "plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='blue', marker='x', label='versicolor')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Iris Dataset (setosa and versicolor)')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "id": "f2236bb5-25fe-491c-8ddf-8e19bf5812c3",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "f2236bb5-25fe-491c-8ddf-8e19bf5812c3"
      },
      "execution_count": null,
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
        "\n",
        "# Standardize the features\n",
        "sc = StandardScaler()\n",
        "X_train_std = sc.fit_transform(X_train)\n",
        "X_test_std = sc.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier\n",
        "svm = SVC(kernel='linear', C=1.0, random_state=1)\n",
        "svm.fit(X_train_std, y_train)\n",
        "\n",
        "# Predict the class labels\n",
        "y_pred = svm.predict(X_test_std)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy"
      ],
      "outputs": []
    },
    {
      "id": "f8142217-b6bc-4850-8675-51f8e3e8336f",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "f8142217-b6bc-4850-8675-51f8e3e8336f"
      },
      "execution_count": null,
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
        "    # Setup marker generator and color map\n",
        "    markers = ('s', 'x', 'o', '^', 'v')\n",
        "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
        "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
        "\n",
        "    # Plot the decision surface\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
        "                           np.arange(x2_min, x2_max, resolution))\n",
        "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
        "    Z = Z.reshape(xx1.shape)\n",
        "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
        "    plt.xlim(xx1.min(), xx1.max())\n",
        "    plt.ylim(xx2.min(), xx2.max())\n",
        "\n",
        "    # Plot class samples\n",
        "    for idx, cl in enumerate(np.unique(y)):\n",
        "        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n",
        "                    alpha=0.8, c=colors[idx],\n",
        "                    marker=markers[idx], label=cl,\n",
        "                    edgecolor='black')\n",
        "\n",
        "    # Highlight test samples\n",
        "    if test_idx:\n",
        "        X_test, y_test = X[test_idx, :], y[test_idx]\n",
        "        plt.scatter(X_test[:, 0], X_test[:, 1],\n",
        "                    c='g', edgecolor='black', alpha=1.0,\n",
        "                    linewidth=1, marker='o',\n",
        "                    s=100, label='test set')\n",
        "\n",
        "# Combine the training and test datasets for visualization\n",
        "X_combined_std = np.vstack((X_train_std, X_test_std))\n",
        "y_combined = np.hstack((y_train, y_test))\n",
        "\n",
        "# Plot the decision regions\n",
        "plot_decision_regions(X_combined_std, y_combined, classifier=svm, test_idx=range(70, 100))\n",
        "plt.xlabel('Sepal length [standardized]')\n",
        "plt.ylabel('Sepal width [standardized]')\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('SVM Classifier on Iris Dataset')\n",
        "plt.show()"
      ],
      "outputs": []
    },
    {
      "id": "67c382ca-e8ef-4b72-b3eb-261fd30e51e3",
      "cell_type": "markdown",
      "source": [
        "## Advantages and Limitations of SVM\n",
        "\n",
        "### Advantages:\n",
        "\n",
        "1. **Effective in High Dimensional Spaces**: SVM works well when the number of features is large.\n",
        "2. **Versatility**: Different kernel functions can be specified for the decision function, making SVM versatile for various datasets.\n",
        "3. **Memory Efficient**: SVM uses only a subset of training points (support vectors) in the decision function.\n",
        "4. **Robust**: SVM is robust to outliers and can produce accurate results even when the data is not linearly separable by using the kernel trick.\n",
        "\n",
        "### Limitations:\n",
        "\n",
        "1. **Sensitive to Kernel Choice**: The choice of kernel and its parameters can greatly influence the performance of SVM.\n",
        "2. **Not Suitable for Large Datasets**: Training an SVM on a large dataset can be computationally intensive and time-consuming.\n",
        "3. **No Direct Probability Estimates**: SVM does not provide direct probability estimates for predictions.\n",
        "\n",
        "In conclusion, SVM is a powerful and versatile classifier that can be used for both linear and non-linear classification tasks. However, it's essential to choose the right kernel and tune the hyperparameters for optimal performance. The Iris dataset example demonstrated the capability of SVM in finding the optimal hyperplane that maximizes the margin between classes."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "67c382ca-e8ef-4b72-b3eb-261fd30e51e3"
      }
    }
  ]
}