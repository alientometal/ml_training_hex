{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "noteable-chatgpt": {
      "create_notebook": {
        "openai_conversation_id": "b445c1de-c6f0-5ee5-81c9-74cd11dbc67f",
        "openai_ephemeral_user_id": "ae887d81-bc3f-5cba-bb4a-0dbd09528325",
        "openai_subdivision1_iso_code": "MX-MOR"
      }
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9",
      "language": "python",
      "name": "python3"
    },
    "selected_hardware_size": "small",
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "id": "0e4575af-8772-424a-b208-7ee76de0b329",
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Data preprocessing is a crucial step in the machine learning pipeline. It involves transforming raw data into a format that can be easily understood and used by machine learning algorithms. Proper preprocessing can significantly improve the performance of machine learning models, while neglecting this step can lead to suboptimal results.\n",
        "\n",
        "In this tutorial, we'll explore various preprocessing techniques and understand their importance in building effective machine learning models."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "0e4575af-8772-424a-b208-7ee76de0b329"
      }
    },
    {
      "id": "6fcac62a-216e-40fc-ae94-d3c01cf1038a",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "6fcac62a-216e-40fc-ae94-d3c01cf1038a"
      },
      "execution_count": null,
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a sample dataframe with missing values\n",
        "data = {\n",
        "    'Age': [25, 30, 35, np.nan, 45],\n",
        "    'Salary': [50000, 55000, np.nan, 62000, 67000],\n",
        "    'Department': ['HR', 'Finance', 'IT', 'Finance', 'IT']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataframe with missing values\n",
        "df"
      ],
      "outputs": []
    },
    {
      "id": "7b354da3-5c3d-48bd-9cea-3849e3481d6b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "7b354da3-5c3d-48bd-9cea-3849e3481d6b"
      },
      "execution_count": null,
      "source": [
        "# Impute missing values\n",
        "imputer_age = SimpleImputer(strategy='mean')\n",
        "df['Age'] = imputer_age.fit_transform(df[['Age']])\n",
        "\n",
        "imputer_salary = SimpleImputer(strategy='median')\n",
        "df['Salary'] = imputer_salary.fit_transform(df[['Salary']])\n",
        "\n",
        "# Display the dataframe after imputation\n",
        "df"
      ],
      "outputs": []
    },
    {
      "id": "1f087c47-9aa2-458b-8c79-2484c370f292",
      "cell_type": "markdown",
      "source": [
        "## Data Scaling and Normalization\n",
        "\n",
        "Many machine learning algorithms are sensitive to the scale of features. For instance, algorithms that rely on distances between data points, like k-means clustering or k-nearest neighbors, can produce different results based on the scale of the features.\n",
        "\n",
        "There are two common methods to scale features:\n",
        "\n",
        "1. **Min-Max Scaling (Normalization)**: This method scales the data to a fixed range, usually [0, 1]. The formula is given by:\n",
        "\n",
        "$$ X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$\n",
        "\n",
        "2. **Standardization (Z-score Normalization)**: This method scales the data such that it has a mean of 0 and a standard deviation of 1. The formula is given by:\n",
        "\n",
        "$$ X_{std} = \\frac{X - \\mu}{\\sigma} $$\n",
        "\n",
        "where $\\mu$ is the mean and $\\sigma$ is the standard deviation.\n",
        "\n",
        "Let's demonstrate these scaling methods using our sample dataframe."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "1f087c47-9aa2-458b-8c79-2484c370f292"
      }
    },
    {
      "id": "3a9166e1-60e8-47fd-8d0a-d78b2db18c7d",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "3a9166e1-60e8-47fd-8d0a-d78b2db18c7d"
      },
      "execution_count": null,
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Min-Max Scaling\n",
        "minmax_scaler = MinMaxScaler()\n",
        "df_minmax = df.copy()\n",
        "df_minmax[['Age', 'Salary']] = minmax_scaler.fit_transform(df[['Age', 'Salary']])\n",
        "\n",
        "# Display the dataframe after Min-Max Scaling\n",
        "df_minmax"
      ],
      "outputs": []
    },
    {
      "id": "035c3c56-0f87-462a-b6fb-787260532c28",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "035c3c56-0f87-462a-b6fb-787260532c28"
      },
      "execution_count": null,
      "source": [
        "# Standardization (Z-score Normalization)\n",
        "standard_scaler = StandardScaler()\n",
        "df_standard = df.copy()\n",
        "df_standard[['Age', 'Salary']] = standard_scaler.fit_transform(df[['Age', 'Salary']])\n",
        "\n",
        "# Display the dataframe after Standardization\n",
        "df_standard"
      ],
      "outputs": []
    },
    {
      "id": "5b7f3b32-b724-423b-b437-1617ff227c3a",
      "cell_type": "markdown",
      "source": [
        "## Encoding Categorical Variables\n",
        "\n",
        "Many machine learning algorithms require numerical input and output variables. However, real-world datasets often contain categorical variables that have string labels rather than numeric values. Encoding these variables is essential to convert them into a format that can be provided to machine learning algorithms.\n",
        "\n",
        "There are several methods to encode categorical variables, including:\n",
        "\n",
        "1. **Label Encoding**: Assigns a unique integer to each category. It's suitable for ordinal variables where the order matters.\n",
        "2. **One-Hot Encoding**: Creates binary columns for each category and indicates the presence (1) or absence (0) of the category. It's suitable for nominal variables where the order doesn't matter.\n",
        "\n",
        "Let's demonstrate these encoding methods using the `Department` column from our sample dataframe."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "5b7f3b32-b724-423b-b437-1617ff227c3a"
      }
    },
    {
      "id": "4aa70265-c182-4996-9223-f40a1c754841",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "4aa70265-c182-4996-9223-f40a1c754841"
      },
      "execution_count": null,
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Label Encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df_label_encoded = df.copy()\n",
        "df_label_encoded['Department'] = label_encoder.fit_transform(df['Department'])\n",
        "\n",
        "# Display the dataframe after Label Encoding\n",
        "df_label_encoded"
      ],
      "outputs": []
    },
    {
      "id": "3e126f20-463e-41f7-92cd-3f733b8cca32",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "3e126f20-463e-41f7-92cd-3f733b8cca32"
      },
      "execution_count": null,
      "source": [
        "# One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder()\n",
        "df_onehot_encoded = df.copy()\n",
        "encoded_columns = onehot_encoder.fit_transform(df[['Department']]).toarray()\n",
        "encoded_df = pd.DataFrame(encoded_columns, columns=onehot_encoder.get_feature_names_out(['Department']))\n",
        "df_onehot_encoded = pd.concat([df_onehot_encoded, encoded_df], axis=1).drop('Department', axis=1)\n",
        "\n",
        "# Display the dataframe after One-Hot Encoding\n",
        "df_onehot_encoded"
      ],
      "outputs": []
    },
    {
      "id": "035837f1-0cf6-4b17-902b-21a0ce98fd2c",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "035837f1-0cf6-4b17-902b-21a0ce98fd2c"
      },
      "execution_count": null,
      "source": [
        "# One-Hot Encoding\n",
        "df_onehot_encoded = df.copy()\n",
        "df_onehot_encoded = pd.get_dummies(df_onehot_encoded, columns=['Department'])\n",
        "\n",
        "# Display the dataframe after One-Hot Encoding\n",
        "df_onehot_encoded"
      ],
      "outputs": []
    },
    {
      "id": "7ac86a9d-aead-4366-a057-bc54fc83f5d7",
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Feature engineering is the process of creating new features or transforming existing ones to enhance the performance of machine learning models. It involves domain knowledge and creativity to design features that capture the underlying patterns in the data.\n",
        "\n",
        "Some common feature engineering techniques include:\n",
        "\n",
        "1. **Polynomial Features**: Creating new features based on polynomial combinations of existing features.\n",
        "2. **Binning**: Dividing continuous features into discrete bins or intervals.\n",
        "3. **Interaction Features**: Creating new features based on interactions between two or more existing features.\n",
        "4. **Feature Decomposition**: Using techniques like PCA (Principal Component Analysis) to reduce the dimensionality of the data.\n",
        "\n",
        "Let's demonstrate feature engineering by creating a new interaction feature between `Age` and `Salary` in our sample dataframe."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "7ac86a9d-aead-4366-a057-bc54fc83f5d7"
      }
    },
    {
      "id": "dff80672-da61-4923-853c-299e98e91c6b",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "dff80672-da61-4923-853c-299e98e91c6b"
      },
      "execution_count": null,
      "source": [
        "# Create an interaction feature between 'Age' and 'Salary'\n",
        "df['Age_Salary_Interaction'] = df['Age'] * df['Salary']\n",
        "\n",
        "# Display the dataframe with the new interaction feature\n",
        "df"
      ],
      "outputs": []
    },
    {
      "id": "991a62c1-146c-4da6-a888-60e533250fa7",
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "\n",
        "Feature selection is the process of selecting a subset of the most relevant features for building machine learning models. It helps in reducing the dimensionality of the data, improving model performance, and reducing overfitting.\n",
        "\n",
        "Some common feature selection techniques include:\n",
        "\n",
        "1. **Filter Methods**: These methods rank features based on statistical measures and select the top features. Examples include correlation coefficient and chi-squared test.\n",
        "2. **Wrapper Methods**: These methods evaluate subsets of features by training a model on each subset and selecting the best performing subset. Examples include forward selection and backward elimination.\n",
        "3. **Embedded Methods**: These methods perform feature selection as part of the model training process. Examples include LASSO and decision trees.\n",
        "\n",
        "Let's demonstrate feature selection using a filter method. We'll rank the features based on their correlation with the `Salary` column and select the top features."
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "991a62c1-146c-4da6-a888-60e533250fa7"
      }
    },
    {
      "id": "e045d333-2b7a-4e33-b3bb-7ff2f571cee2",
      "cell_type": "code",
      "metadata": {
        "noteable": {
          "cell_type": "code"
        },
        "id": "e045d333-2b7a-4e33-b3bb-7ff2f571cee2"
      },
      "execution_count": null,
      "source": [
        "# Compute the correlation of each feature with the 'Salary' column\n",
        "correlation_with_salary = df.corr()['Salary']\n",
        "\n",
        "# Display the correlation values\n",
        "correlation_with_salary"
      ],
      "outputs": []
    },
    {
      "id": "ed188227-c118-4feb-a1fe-ec2883009cae",
      "cell_type": "markdown",
      "source": [
        "## Conclusion and Key Takeaways\n",
        "\n",
        "Data preprocessing is a critical step in the machine learning pipeline. Proper preprocessing can significantly enhance the performance of machine learning models. Here are the key takeaways from this tutorial:\n",
        "\n",
        "- **Handling Missing Data**: Use techniques like imputation or predictive modeling to handle missing values.\n",
        "- **Data Scaling and Normalization**: Scale features to ensure that they are on a similar scale, especially for algorithms sensitive to feature scales.\n",
        "- **Encoding Categorical Variables**: Convert categorical data into a numerical format using methods like label encoding or one-hot encoding.\n",
        "- **Feature Engineering**: Create new features or transform existing ones to capture underlying patterns in the data.\n",
        "- **Feature Selection**: Select a subset of the most relevant features to reduce dimensionality and improve model performance.\n",
        "\n",
        "It's essential to understand the nature of your data and the requirements of the machine learning algorithms you plan to use. Different algorithms might require different preprocessing steps. Always experiment with various preprocessing techniques and evaluate their impact on model performance.\n",
        "\n",
        "Happy preprocessing!"
      ],
      "metadata": {
        "noteable": {
          "cell_type": "markdown"
        },
        "id": "ed188227-c118-4feb-a1fe-ec2883009cae"
      }
    }
  ]
}